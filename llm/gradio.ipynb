{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False) # temperature가 커질수록 자유도가 올라간다(자유도가 없다 = 입력 그대로 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 기록을 포함하여 응답을 생성하는 함수\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ChatOllama 형식으로 변환\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # 현재 메시지 추가\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = model.invoke(chat_history)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jojungon/AI_Project/llm/lib/python3.10/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"안녕하세요!\",\n",
    "        \"인공지능에 대해 설명해주세요.\",\n",
    "        \"파이썬의 장점은 무엇인가요?\"\n",
    "    ],\n",
    "    title=\"AI 챗봇\",\n",
    "    description=\"질문을 입력하면 AI가 답변합니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7860, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제(Gradio + csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>한국폴리텍대학 스마트금융과의 최종 아웃풋은 어떤건가요?</td>\n",
       "      <td>스마트금융과는 찍어내기식의 포트폴리오가 아니라 매년 업체에서 요구하는 기술 및 주제...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>한국폴리텍대학 스마트금융과의 최종 포트폴리오는 어떤건가요?</td>\n",
       "      <td>유튜브 채널에서 스마트금융과를 검색하시면 한국폴리텍대학 스마트금융과 포트폴리오 발표...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?</td>\n",
       "      <td>영문 타자연습 및 스마트금융과에 대한 열정을 보여주면 좋다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>한국폴리텍대학 스마트금융과 입학 전까지 어떤걸 공부하면 될까요?</td>\n",
       "      <td>기본적인 OA를 잘 다루고 기본코드는 HKCODE의 기본 내용은 보고오면 됨. 파이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>한국폴리텍대학 스마트금융과는 대면/비대면 수업 어떻게 진행되나요?</td>\n",
       "      <td>대면으로 진행합니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   inputs  \\\n",
       "27         한국폴리텍대학 스마트금융과의 최종 아웃풋은 어떤건가요?   \n",
       "28       한국폴리텍대학 스마트금융과의 최종 포트폴리오는 어떤건가요?   \n",
       "29  한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?   \n",
       "30    한국폴리텍대학 스마트금융과 입학 전까지 어떤걸 공부하면 될까요?   \n",
       "31   한국폴리텍대학 스마트금융과는 대면/비대면 수업 어떻게 진행되나요?   \n",
       "\n",
       "                                             response  \n",
       "27  스마트금융과는 찍어내기식의 포트폴리오가 아니라 매년 업체에서 요구하는 기술 및 주제...  \n",
       "28  유튜브 채널에서 스마트금융과를 검색하시면 한국폴리텍대학 스마트금융과 포트폴리오 발표...  \n",
       "29                   영문 타자연습 및 스마트금융과에 대한 열정을 보여주면 좋다  \n",
       "30  기본적인 OA를 잘 다루고 기본코드는 HKCODE의 기본 내용은 보고오면 됨. 파이...  \n",
       "31                                        대면으로 진행합니다.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"../dataset/indata_kor.csv\", encoding='CP949')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_76771/2819514520.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 임베딩 모델 초기화\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 데이터베이스 생성\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_76771/3071692779.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"gemma2\", tempeature=0.1)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "llm = ChatOllama(model=\"gemma2\", tempeature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 함수 정의\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ConversationalRetrievalChain 형식으로 변환\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # 소스 문서 정보 추출\n",
    "    sources = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\n참고 출처: {', '.join(sources)}\" if sources else \"\"\n",
    "\n",
    "    return response['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jojungon/AI_Project/llm/lib/python3.10/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?\",\n",
    "        \"스마트금융과에 대해 설명해주세요\",\n",
    "        \"한국폴리텍대한 추천할만한 학과 하나를 소개해주세요.\"\n",
    "    ],\n",
    "    title=\"대학 정보 AI 챗봇\",\n",
    "    description=\"스마트금융과에 대한 질문을 입력하면 AI가 CSV데이터를 참고하여 한글로 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_76771/4160252103.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7862, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제(인터넷 URL정보 요약)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents\n",
    "\n",
    "def format_docs(docs):\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(url, question):\n",
    "    try: \n",
    "        retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "        # 검색된 문서가 비어있는지 확인\n",
    "        if not retrieved_docs:\n",
    "            raise ValueError(\"No documents retrieved. Please check the retrieval process.\")\n",
    "\n",
    "        # 문서 형식 지정\n",
    "        formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "        # # 만약 formatted_context가 너무 길면 트림 처리 등을 추가할 수 있음\n",
    "        # if len(formatted_context) > 1000:\n",
    "        #     formatted_context = formatted_context[:1000]  # 예시로 1000자 이내로 자르기\n",
    "\n",
    "        # 질문과 문맥을 합쳐서 프롬프트 생성\n",
    "        formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "\n",
    "        # 요청 본문에 'data' 필드를 추가하여 API 호출\n",
    "        response = ollama.chat(model='gemma2', messages=[{\n",
    "            'role': 'user', \n",
    "            'content': formatted_prompt,\n",
    "            'data': {'model': 'gemma2', 'prompt': formatted_prompt}  # 'data' 필드를 추가\n",
    "        }])\n",
    "\n",
    "        # 응답이 정상적인지 확인\n",
    "        if 'message' not in response or 'content' not in response['message']:\n",
    "            raise ValueError(\"Invalid response from model. 'message' or 'content' missing.\")\n",
    "        \n",
    "        # 모델 응답 반환\n",
    "        return response['message']['content']\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류가 발생하면 로그로 출력하고 None 반환\n",
    "        logging.error(f\"Error in rag_chain: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradio interface\n",
    "\n",
    "# iface = gr.Interface(\n",
    "\n",
    "#     fn=rag_chain,\n",
    "\n",
    "#     inputs=[\"text\", \"text\"],\n",
    "\n",
    "#     outputs=\"text\",\n",
    "\n",
    "#     title=\"RAG Chain Question Answering\",\n",
    "\n",
    "#     description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Tabbed Interface\n",
    "with gr.Blocks() as iface:\n",
    "    # Tab for Question and Answer\n",
    "    with gr.Tab(\"질문과 답변\"):\n",
    "        gr.Interface(\n",
    "            fn=rag_chain,\n",
    "            inputs=[\"text\", \"text\"],\n",
    "            outputs=\"text\",\n",
    "            title=\"RAG Chain Question Answering\",\n",
    "            description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "        ).render()\n",
    "\n",
    "    # Tab for Visualization (Word Cloud)\n",
    "    with gr.Tab(\"시각화 (워드클라우드)\"):\n",
    "        gr.Markdown(\"이 탭은 시각화를 위한 공간입니다. 워드클라우드 기능이 여기에 추가될 예정입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 디버그 모드로 Gradio 인터페이스 실행\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43miface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7861\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Project/llm/lib/python3.10/site-packages/gradio/blocks.py:2618\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;66;03m# If running in a colab or not able to access localhost,\u001b[39;00m\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;66;03m# a shareable link must be created.\u001b[39;00m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2613\u001b[0m     _frontend\n\u001b[1;32m   2614\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wasm_utils\u001b[38;5;241m.\u001b[39mIS_WASM\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m networking\u001b[38;5;241m.\u001b[39murl_ok(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url)\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare\n\u001b[1;32m   2617\u001b[0m ):\n\u001b[0;32m-> 2618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_colab \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "\u001b[0;31mValueError\u001b[0m: When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost."
     ]
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(SST:음성을 텍스트로 전환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install openai-whisper\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import whisper #type: ignore\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .env 파일에서 환경변수 로드(필요한 경우)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg 경로 명시적 설정\n",
    "# os.envrion[\"FFMPEG_BINARY\"] = \"C:/aiproject/ffmpeg/bin/ffmpeg.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"/opt/homebrew/bin\"\n",
    "os.environ[\"FFMPEG_BINARY\"] = r\"/opt/homebrew/bin/ffmpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Whisper 모델 로드\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # 오디오 파일 전사\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # 전사된 텍스트 반환\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"오디오 파일을 업로드 해주세요.\"\n",
    "    try:\n",
    "        transcribe_text = transcribe_audio(audio)\n",
    "        return transcribe_text\n",
    "    except Exception as e:\n",
    "        return  f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"MP3 파일 업로드\"),\n",
    "    outputs=\"text\",\n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description=\"MP3 파일을 업로드하면 텍스트로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제(TTS: 텍스트를 음성변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install gtts\n",
    "import gradio as gr\n",
    "from gtts import gTTS #type: ignore\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang=\"ko\"):\n",
    "    # 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as fp:\n",
    "        temp_filename = fp.name\n",
    "\n",
    "    # TTS 변환\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생 또는 다운로드 할 수 있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_tts,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=5, label=\"텍스트 입력\"),\n",
    "        gr.Dropdown(choices=['ko', 'en', 'ja', 'zh-cn'], label=\"언어 선택\", value='ko')\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"생성된 오디오\"),\n",
    "        gr.Textbox(label=\"상태 메시지\")\n",
    "    ],\n",
    "    title = \"Text to Speech Converter\",\n",
    "    description=\"텍스트를 입력하연 MP3 파일로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:11434\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:11434/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=11434, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 11434\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 분류 예제(Gradio 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow MobileNetV2 모델 로드\n",
    "model = tf.keras.applications.MobileNetV2(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_url):\n",
    "    try:\n",
    "        # URL에서 이미지 가져오기\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).resize((224, 224)) # BytesIO 사용하여 이미지 열기\n",
    "\n",
    "        # 이미지를 배열로 변환\n",
    "        image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image_array = tf.expand_dims(image_array, axis=0) # 배치 차원 추가\n",
    "        image_array = tf.keras.applications.mobilenet_v2.preprocess_input(image_array) # 전처리\n",
    "\n",
    "        # 예측 수행\n",
    "        predictions = model.predict(image_array)\n",
    "        decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predcitions(predictions, top=3)[0] # 상위 3개 예측\n",
    "\n",
    "        # Gradio Label 컴포넌트에 맞게 결과 형식 변경\n",
    "        # {label1: confidence1, label2: confidence2, ...} 형식으로 반환\n",
    "        result = {label: float(prob) for (_, label, prob) in decoded_predictions}\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": 1.0} # 에러 발생 시 기본값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=predict_image,\n",
    "    inputs=gr.Textbox(label=\"이미지 URL 입력\"),\n",
    "    outputs=gr.Label(num_top_classes=3, label=\"예측 결과\"),\n",
    "    title=\"음식 이미지 분류\",\n",
    "    description=\"이미지 URL을 입력하면 상위 3개의 예측 결과를 확률과 함께 표시합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)\n",
    "\n",
    "# 예시 이미지 URL : https://health.chosun.com/site/data/img_dir/2024/04/19/2024041901914_0.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이미지 분류 예제(Gradio + gemma2 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow MobileNetV2 모델 로드\n",
    "model = tf.keras.applications.MobileNetV2(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_SERVER = \"http://localhost:11434\"  # 로컬 서버 주소\n",
    "MODEL_NAME = \"gemma2\"  # 사용하려는 Ollama 모델 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama를 사용해 음식 설명 생성\n",
    "def get_food_description_with_langchain(food_name):\n",
    "    \"\"\"\n",
    "    LangChain ChatOllama를 사용하여 음식 설명 생성\n",
    "    \"\"\"\n",
    "    try:\n",
    "        chat = ChatOllama(base_url=OLLAMA_SERVER, model=MODEL_NAME)\n",
    "        prompt = f\"Tell me about {food_name}.\"\n",
    "        response = chat.predict(prompt)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Failed to retrieve description: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 예측 함수\n",
    "def predict_image_with_description(image_url):\n",
    "    \"\"\"\n",
    "    이미지 URL을 받아 음식 예측과 Ollama 설명을 반환\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # URL에서 이미지 가져오기\n",
    "        response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(response.content)).resize((224, 224))  # BytesIO 사용하여 이미지 열기\n",
    "\n",
    "        # 이미지를 배열로 변환\n",
    "        image_array = tf.keras.preprocessing.image.img_to_array(image)\n",
    "        image_array = tf.expand_dims(image_array, axis=0)  # 배치 차원 추가\n",
    "        image_array = tf.keras.applications.mobilenet_v2.preprocess_input(image_array)  # 전처리\n",
    "\n",
    "        # 예측 수행\n",
    "        predictions = model.predict(image_array)\n",
    "        decoded_predictions = tf.keras.applications.mobilenet_v2.decode_predictions(predictions, top=3)[0]  # 상위 3개 예측 결과 반환\n",
    "\n",
    "        # 예측 결과 형식화\n",
    "        result = {label: float(prob) for (_, label, prob) in decoded_predictions}\n",
    "\n",
    "        # 가장 높은 확률의 예측값으로 Ollama 설명 생성\n",
    "        top_food = decoded_predictions[0][1]  # 가장 확률이 높은 음식 이름\n",
    "        description = get_food_description_with_langchain(top_food)\n",
    "\n",
    "        return result, description  # 예측 결과와 Ollama 설명 반환\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"error\": 1.0}, f\"Error: {e}\"  # 에러 발생 시 기본값 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=predict_image_with_description,\n",
    "    inputs=gr.Textbox(label=\"이미지 URL 입력\"),\n",
    "    outputs=[\n",
    "        gr.Label(num_top_classes=3, label=\"예측 결과\"),  # 상위 3개 예측 결과\n",
    "        gr.Textbox(label=\"음식 설명\", interactive=False)  # Ollama로 생성한 설명 출력\n",
    "    ],\n",
    "    title=\"음식 이미지 분류 및 설명 생성기\",\n",
    "    description=\"이미지 URL을 입력하면 음식 분류 결과와 설명을 제공합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 296ms/step\n",
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
      "\u001b[1m35363/35363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_97705/2595182604.py:7: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  chat = ChatOllama(base_url=OLLAMA_SERVER, model=MODEL_NAME)\n",
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_97705/2595182604.py:9: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chat.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기타(워드클라우드 gradio 예제)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 클라우드 생성함수\n",
    "def generate_wordcloud(file_obj):\n",
    "    try:\n",
    "        # 파일이 없는 경우 처리\n",
    "        if file_obj is None:\n",
    "            return None\n",
    "        \n",
    "        # Gradio의 파일 객체에서 파일 경로 가져오기\n",
    "        file_path = file_obj.name\n",
    "        \n",
    "        # 파일 읽기\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # 워드클라우드 생성\n",
    "        wordcloud = WordCloud(\n",
    "            font_path=\"NanumGothic\", # 한글 폰트 설정\n",
    "            background_color=\"white\",\n",
    "            width=800,\n",
    "            height=600,\n",
    "            max_words=200,\n",
    "            max_font_size=100,\n",
    "            min_font_size=10,\n",
    "            random_state=42\n",
    "        ).generate(text)\n",
    "        \n",
    "        # 워드클라우드 이미지 플롯\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 결과 이미지 저장\n",
    "        output_path = \"wordcloud.png\"\n",
    "        plt.savefig(output_path)\n",
    "        plt.close() # 메모리 누수 방지를 위해 figure 닫기\n",
    "        return output_path\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=generate_wordcloud,\n",
    "    inputs=gr.File(label=\"Upload a .txt file\"),\n",
    "    outputs=gr.Image(type=\"filepath\", label=\"Word Cloud\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* Running on public URL: https://051312234ca73f08fc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://051312234ca73f08fc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iface.launch(server_port=7861, share=True, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자기소개서 도우미 챗봇 예제(Gradio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install\n",
    "import os\n",
    "import gradio as gr\n",
    "from langchain.llms import Ollama\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_25041/2509825096.py:3: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_model = Ollama(model=\"gemma2\")\n"
     ]
    }
   ],
   "source": [
    "# Ollama 설정 (Gemma2 모델 사용)\n",
    "os.environ[\"OLLAMA_API_BASE\"] = \"http://localhost:11434\"  # Ollama 서버 주소\n",
    "ollama_model = Ollama(model=\"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 템플릿 설정\n",
    "TEMPLATES = {\n",
    "    \"취업\": \"Based on the following keywords and example, write a personal statement for a job application.\",\n",
    "    \"대학원\": \"Using the provided keywords, draft a personal statement for a graduate school application.\",\n",
    "    \"봉사활동\": \"With the given keywords, write a personal statement emphasizing volunteer experience and motivation.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 언어 지원: 한국어, 영어, 일본어\n",
    "LANGUAGES = {\n",
    "    \"한국어\": \"Please write the response in Korean.\",\n",
    "    \"영어\": \"Please write the response in English.\",\n",
    "    \"일본어\": \"Please write the response in Japanese.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동 키워드 추천 함수\n",
    "def recommend_keywords(purpose):\n",
    "    if purpose == \"취업\":\n",
    "        return \"책임감, 팀워크, 문제 해결 능력\"\n",
    "    elif purpose == \"대학원\":\n",
    "        return \"연구 열정, 창의력, 학업 성취도\"\n",
    "    elif purpose == \"봉사활동\":\n",
    "        return \"사회적 책임감, 희생정신, 리더십\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자기소개서 작성 함수\n",
    "def generate_statement(purpose, language, keywords, example_sentence=None):\n",
    "    if purpose not in TEMPLATES:\n",
    "        return \"❌ 지원 목적을 올바르게 선택해주세요.\"\n",
    "    if language not in LANGUAGES:\n",
    "        return \"❌ 언어를 올바르게 선택해주세요.\"\n",
    "\n",
    "    # 템플릿 생성\n",
    "    template = TEMPLATES[purpose] + \"\\n\\nKeywords: {keywords}\\n\" + LANGUAGES[language]\n",
    "    if example_sentence:\n",
    "        template += f\"\\n\\nExample sentence: {example_sentence}\"\n",
    "\n",
    "    prompt = PromptTemplate(input_variables=[\"keywords\"], template=template)\n",
    "    chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
    "    response = chain.run({\"keywords\": keywords})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF 저장 함수\n",
    "def save_to_pdf(statement, filename=\"personal_statement.pdf\"):\n",
    "    pdf = FPDF()\n",
    "    pdf.add_page()\n",
    "    pdf.add_font('NanumGothic', '', r'C:\\Windows\\Fonts\\malgun.ttf', uni=True)  # '맑은 고딕' 폰트 경로 설정\n",
    "    pdf.set_font('NanumGothic', size=12)  # 폰트 설정\n",
    "    pdf.multi_cell(0, 10, statement)\n",
    "    pdf.output(filename)\n",
    "    return f\"✔️ PDF 저장 완료: {filename}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스\n",
    "def chatbot_interface(purpose, language, keywords, example_sentence=None, save_pdf=False):\n",
    "    statement = generate_statement(purpose, language, keywords, example_sentence)\n",
    "    if save_pdf:\n",
    "        save_to_pdf(statement)\n",
    "    return statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# 📝 다목적 자기소개서 작성 도우미\")\n",
    "    gr.Markdown(\"키워드와 추천 문장을 활용하여 취업, 대학원, 봉사활동 자기소개서를 생성하고 PDF로 저장하세요!\")\n",
    "\n",
    "    # 입력 영역\n",
    "    with gr.Row():\n",
    "        purpose_input = gr.Dropdown(label=\"지원 목적\", choices=[\"취업\", \"대학원\", \"봉사활동\"], value=\"취업\")\n",
    "        language_input = gr.Dropdown(label=\"언어 선택\", choices=[\"한국어\", \"영어\", \"일본어\"], value=\"한국어\")\n",
    "\n",
    "    recommended_keywords = gr.Textbox(label=\"추천 키워드\", interactive=False)\n",
    "    recommend_btn = gr.Button(\"키워드 추천\")\n",
    "    recommend_btn.click(recommend_keywords, inputs=[purpose_input], outputs=[recommended_keywords])\n",
    "\n",
    "    with gr.Row():\n",
    "        keywords_input = gr.Textbox(label=\"사용자 키워드 입력\", placeholder=\"예: 책임감, 팀워크, 문제 해결 능력\")\n",
    "        example_sentence_input = gr.Textbox(\n",
    "            label=\"추천 문장 (선택 사항)\",\n",
    "            placeholder=\"예: '저는 도전을 두려워하지 않고 성공적으로 프로젝트를 완수했습니다.'\"\n",
    "        )\n",
    "\n",
    "    save_pdf_toggle = gr.Checkbox(label=\"PDF로 저장\", value=False)\n",
    "\n",
    "    # 출력 영역\n",
    "    output = gr.Textbox(label=\"작성된 자기소개서\", lines=6)\n",
    "    submit_btn = gr.Button(\"작성하기\")\n",
    "    submit_btn.click(\n",
    "        fn=chatbot_interface,\n",
    "        inputs=[purpose_input, language_input, keywords_input, example_sentence_input, save_pdf_toggle],\n",
    "        outputs=[output]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_25041/4139684956.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=ollama_model, prompt=prompt)\n",
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_25041/4139684956.py:15: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run({\"keywords\": keywords})\n"
     ]
    }
   ],
   "source": [
    "# 실행\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "# 종료(자원회수)\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
