{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "model = ChatOllama(model=\"gemma2\", temperature=0.7, verbose=False) # temperature가 커질수록 자유도가 올라간다(자유도가 없다 = 입력 그대로 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 기록을 포함하여 응답을 생성하는 함수\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ChatOllama 형식으로 변환\n",
    "    chat_history = []\n",
    "    for human, ai in history:\n",
    "        chat_history.append(HumanMessage(content=human))\n",
    "        chat_history.append(AIMessage(content=ai))\n",
    "\n",
    "    # 현재 메시지 추가\n",
    "    chat_history.append(HumanMessage(content=message))\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = model.invoke(chat_history)\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jojungon/AI_Project/llm/lib/python3.10/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"안녕하세요!\",\n",
    "        \"인공지능에 대해 설명해주세요.\",\n",
    "        \"파이썬의 장점은 무엇인가요?\"\n",
    "    ],\n",
    "    title=\"AI 챗봇\",\n",
    "    description=\"질문을 입력하면 AI가 답변합니다.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7860, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제(Gradio + csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>한국폴리텍대학 스마트금융과의 최종 아웃풋은 어떤건가요?</td>\n",
       "      <td>스마트금융과는 찍어내기식의 포트폴리오가 아니라 매년 업체에서 요구하는 기술 및 주제...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>한국폴리텍대학 스마트금융과의 최종 포트폴리오는 어떤건가요?</td>\n",
       "      <td>유튜브 채널에서 스마트금융과를 검색하시면 한국폴리텍대학 스마트금융과 포트폴리오 발표...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?</td>\n",
       "      <td>영문 타자연습 및 스마트금융과에 대한 열정을 보여주면 좋다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>한국폴리텍대학 스마트금융과 입학 전까지 어떤걸 공부하면 될까요?</td>\n",
       "      <td>기본적인 OA를 잘 다루고 기본코드는 HKCODE의 기본 내용은 보고오면 됨. 파이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>한국폴리텍대학 스마트금융과는 대면/비대면 수업 어떻게 진행되나요?</td>\n",
       "      <td>대면으로 진행합니다.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   inputs  \\\n",
       "27         한국폴리텍대학 스마트금융과의 최종 아웃풋은 어떤건가요?   \n",
       "28       한국폴리텍대학 스마트금융과의 최종 포트폴리오는 어떤건가요?   \n",
       "29  한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?   \n",
       "30    한국폴리텍대학 스마트금융과 입학 전까지 어떤걸 공부하면 될까요?   \n",
       "31   한국폴리텍대학 스마트금융과는 대면/비대면 수업 어떻게 진행되나요?   \n",
       "\n",
       "                                             response  \n",
       "27  스마트금융과는 찍어내기식의 포트폴리오가 아니라 매년 업체에서 요구하는 기술 및 주제...  \n",
       "28  유튜브 채널에서 스마트금융과를 검색하시면 한국폴리텍대학 스마트금융과 포트폴리오 발표...  \n",
       "29                   영문 타자연습 및 스마트금융과에 대한 열정을 보여주면 좋다  \n",
       "30  기본적인 OA를 잘 다루고 기본코드는 HKCODE의 기본 내용은 보고오면 됨. 파이...  \n",
       "31                                        대면으로 진행합니다.  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CSV 파일 로드\n",
    "df = pd.read_csv(\"../dataset/indata_kor.csv\", encoding='CP949')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 분할\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_text(\"\\n\".join(df.to_string()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_76771/2819514520.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 임베딩 모델 초기화\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벡터 데이터베이스 생성\n",
    "vectorstore = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_76771/3071692779.py:2: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"gemma2\", tempeature=0.1)\n"
     ]
    }
   ],
   "source": [
    "# ChatOllama 모델 초기화\n",
    "llm = ChatOllama(model=\"gemma2\", tempeature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm,\n",
    "    vectorstore.as_retriever(search_kwargs={\"k\":1}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 채팅 함수 정의\n",
    "def chat(message, history):\n",
    "    # 이전 대화 기록을 ConversationalRetrievalChain 형식으로 변환\n",
    "    chat_history = [(human, ai) for human, ai in history]\n",
    "\n",
    "    # 모델을 사용하여 응답 생성\n",
    "    response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n",
    "\n",
    "    # 소스 문서 정보 추출\n",
    "    sources = set([doc.metadata.get('source', 'Unknown') for doc in response['source_documents']])\n",
    "    source_info = f\"\\n\\n참고 출처: {', '.join(sources)}\" if sources else \"\"\n",
    "\n",
    "    return response['answer'] + source_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jojungon/AI_Project/llm/lib/python3.10/site-packages/gradio/components/chatbot.py:279: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Gradio 인터페이스 설정\n",
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"한국폴리텍대학 스마트금융과 면접시에는 어떤걸 준비하고 가면 될까요?\",\n",
    "        \"스마트금융과에 대해 설명해주세요\",\n",
    "        \"한국폴리텍대한 추천할만한 학과 하나를 소개해주세요.\"\n",
    "    ],\n",
    "    title=\"대학 정보 AI 챗봇\",\n",
    "    description=\"스마트금융과에 대한 질문을 입력하면 AI가 CSV데이터를 참고하여 한글로 답변합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t5/w4syjrms3gn00d04kqv8dbyr0000gn/T/ipykernel_76771/4160252103.py:7: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa_chain({\"question\": message, \"chat_history\": chat_history})\n"
     ]
    }
   ],
   "source": [
    "# 서버 실행\n",
    "demo.launch(server_port=7862, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제(인터넷 URL정보 요약)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=(url,),\n",
    "        bs_kwargs=dict()\n",
    "    )\n",
    "\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "\n",
    "    # vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents\n",
    "\n",
    "def format_docs(docs):\n",
    "\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(url, question):\n",
    "    try: \n",
    "        retriever = load_and_retrieve_docs(url)\n",
    "\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "        # 검색된 문서가 비어있는지 확인\n",
    "        if not retrieved_docs:\n",
    "            raise ValueError(\"No documents retrieved. Please check the retrieval process.\")\n",
    "\n",
    "        # 문서 형식 지정\n",
    "        formatted_context = format_docs(retrieved_docs)\n",
    "\n",
    "        # # 만약 formatted_context가 너무 길면 트림 처리 등을 추가할 수 있음\n",
    "        # if len(formatted_context) > 1000:\n",
    "        #     formatted_context = formatted_context[:1000]  # 예시로 1000자 이내로 자르기\n",
    "\n",
    "        # 질문과 문맥을 합쳐서 프롬프트 생성\n",
    "        formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\"\n",
    "\n",
    "        # 요청 본문에 'data' 필드를 추가하여 API 호출\n",
    "        response = ollama.chat(model='gemma2', messages=[{\n",
    "            'role': 'user', \n",
    "            'content': formatted_prompt,\n",
    "            'data': {'model': 'gemma2', 'prompt': formatted_prompt}  # 'data' 필드를 추가\n",
    "        }])\n",
    "\n",
    "        # 응답이 정상적인지 확인\n",
    "        if 'message' not in response or 'content' not in response['message']:\n",
    "            raise ValueError(\"Invalid response from model. 'message' or 'content' missing.\")\n",
    "        \n",
    "        # 모델 응답 반환\n",
    "        return response['message']['content']\n",
    "\n",
    "    except Exception as e:\n",
    "        # 오류가 발생하면 로그로 출력하고 None 반환\n",
    "        logging.error(f\"Error in rag_chain: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gradio interface\n",
    "\n",
    "# iface = gr.Interface(\n",
    "\n",
    "#     fn=rag_chain,\n",
    "\n",
    "#     inputs=[\"text\", \"text\"],\n",
    "\n",
    "#     outputs=\"text\",\n",
    "\n",
    "#     title=\"RAG Chain Question Answering\",\n",
    "\n",
    "#     description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio Tabbed Interface\n",
    "with gr.Blocks() as iface:\n",
    "    # Tab for Question and Answer\n",
    "    with gr.Tab(\"질문과 답변\"):\n",
    "        gr.Interface(\n",
    "            fn=rag_chain,\n",
    "            inputs=[\"text\", \"text\"],\n",
    "            outputs=\"text\",\n",
    "            title=\"RAG Chain Question Answering\",\n",
    "            description=\"Enter a URL and a query to get answers from the RAG chain.\"\n",
    "        ).render()\n",
    "\n",
    "    # Tab for Visualization (Word Cloud)\n",
    "    with gr.Tab(\"시각화 (워드클라우드)\"):\n",
    "        gr.Markdown(\"이 탭은 시각화를 위한 공간입니다. 워드클라우드 기능이 여기에 추가될 예정입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rerunning server... use `close()` to stop if you need to change `launch()` parameters.\n",
      "----\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 디버그 모드로 Gradio 인터페이스 실행\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43miface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7861\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m0.0.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/AI_Project/llm/lib/python3.10/site-packages/gradio/blocks.py:2618\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, max_file_size, enable_monitoring, strict_cors, node_server_name, node_port, ssr_mode, pwa, _frontend)\u001b[0m\n\u001b[1;32m   2610\u001b[0m \u001b[38;5;66;03m# If running in a colab or not able to access localhost,\u001b[39;00m\n\u001b[1;32m   2611\u001b[0m \u001b[38;5;66;03m# a shareable link must be created.\u001b[39;00m\n\u001b[1;32m   2612\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2613\u001b[0m     _frontend\n\u001b[1;32m   2614\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wasm_utils\u001b[38;5;241m.\u001b[39mIS_WASM\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m networking\u001b[38;5;241m.\u001b[39murl_ok(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url)\n\u001b[1;32m   2616\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare\n\u001b[1;32m   2617\u001b[0m ):\n\u001b[0;32m-> 2618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_colab \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "\u001b[0;31mValueError\u001b[0m: When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost."
     ]
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 예제(SST:음성을 텍스트로 전환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install openai-whisper\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import whisper #type: ignore\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .env 파일에서 환경변수 로드(필요한 경우)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ffmpeg 경로 명시적 설정\n",
    "# os.envrion[\"FFMPEG_BINARY\"] = \"C:/aiproject/ffmpeg/bin/ffmpeg.exe\"\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"/opt/homebrew/bin\"\n",
    "os.environ[\"FFMPEG_BINARY\"] = r\"/opt/homebrew/bin/ffmpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(audio_path):\n",
    "    # Whisper 모델 로드\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # 오디오 파일 전사\n",
    "    result = model.transcribe(audio_path)\n",
    "\n",
    "    # 전사된 텍스트 반환\n",
    "    return result[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(audio):\n",
    "    if audio is None:\n",
    "        return \"오디오 파일을 업로드 해주세요.\"\n",
    "    try:\n",
    "        transcribe_text = transcribe_audio(audio)\n",
    "        return transcribe_text\n",
    "    except Exception as e:\n",
    "        return  f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_audio,\n",
    "    inputs=gr.Audio(type=\"filepath\", label=\"MP3 파일 업로드\"),\n",
    "    outputs=\"text\",\n",
    "    title = \"MP3 to Text Converter\",\n",
    "    description=\"MP3 파일을 업로드하면 텍스트로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot 예제(TTS: 텍스트를 음성변환)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 설치 : pip install gtts\n",
    "import gradio as gr\n",
    "from gtts import gTTS #type: ignore\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang=\"ko\"):\n",
    "    # 임시 파일 생성\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as fp:\n",
    "        temp_filename = fp.name\n",
    "\n",
    "    # TTS 변환\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생 또는 다운로드 할 수 있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio 인터페이스 생성\n",
    "iface = gr.Interface(\n",
    "    fn=process_tts,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=5, label=\"텍스트 입력\"),\n",
    "        gr.Dropdown(choices=['ko', 'en', 'ja', 'zh-cn'], label=\"언어 선택\", value='ko')\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Audio(label=\"생성된 오디오\"),\n",
    "        gr.Textbox(label=\"상태 메시지\")\n",
    "    ],\n",
    "    title = \"Text to Speech Converter\",\n",
    "    description=\"텍스트를 입력하연 MP3 파일로 변환합니다.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:11434\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:11434/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=11434, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 11434\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
