{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import bs4\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "import logging\n",
    "from gtts import gTTS #type: ignore\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load, split, and retrieve documents\n",
    "def load_and_retrieve_docs(url):\n",
    "    loader = WebBaseLoader(web_paths=(url,), bs_kwargs=dict())\n",
    "    docs = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    embeddings = OllamaEmbeddings(model=\"gemma2\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(text, lang=\"ko\"):\n",
    "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\") as fp:\n",
    "        temp_filename = fp.name\n",
    "    tts = gTTS(text=text, lang=lang)\n",
    "    tts.save(temp_filename)\n",
    "    return temp_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tts(text, lang):\n",
    "    if not text:\n",
    "        return None, \"텍스트를 입력해주세요.\"\n",
    "    try:\n",
    "        audio_file = text_to_speech(text, lang)\n",
    "        return audio_file, \"변환이 완료되었습니다. 아래에서 재생 또는 다운로드 할 수 있습니다.\"\n",
    "    except Exception as e:\n",
    "        return None, f\"오류가 발생했습니다: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to format documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(url, question):\n",
    "    try:\n",
    "        retriever = load_and_retrieve_docs(url)\n",
    "        retrieved_docs = retriever.invoke(question)\n",
    "        if not retrieved_docs:\n",
    "            raise ValueError(\"No documents retrieved. Please check the retrieval process.\")\n",
    "        \n",
    "        formatted_context = format_docs(retrieved_docs)\n",
    "        formatted_prompt = f\"Question: {question}\\n\\nContext: {formatted_context}\\n\\nPlease answer in Korean.\"\n",
    "        \n",
    "        # 모델에게 질문을 보내 응답을 받음\n",
    "        response = ollama.chat(model='gemma2', messages=[{\n",
    "            'role': 'user',\n",
    "            'content': formatted_prompt\n",
    "        }])\n",
    "        \n",
    "        if 'message' not in response or 'content' not in response['message']:\n",
    "            raise ValueError(\"Invalid response from model. 'message' or 'content' missing.\")\n",
    "        \n",
    "        # 정상적인 경우, 두 값을 반환\n",
    "        result = response['message']['content']\n",
    "        return result, result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in rag_chain: {e}\")\n",
    "        # 에러 발생 시, 기본값으로 두 값을 반환\n",
    "        return \"오류가 발생했습니다. URL 또는 질문을 확인해주세요.\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as iface:\n",
    "    shared_state = gr.State(\"\")  # Shared content state\n",
    "\n",
    "    # Tab 1: 질문과 답변\n",
    "    with gr.Tab(\"질문과 답변\"):\n",
    "        url_input = gr.Textbox(label=\"URL 입력\", placeholder=\"Enter a URL\")\n",
    "        query_input = gr.Textbox(label=\"요청사항\", placeholder=\"Enter your question\")\n",
    "        answer_output = gr.Textbox(label=\"요청결과\", interactive=False)\n",
    "        generate_button = gr.Button(\"요청하기\")\n",
    "\n",
    "        # RAG Chain 호출 - 답변 생성 버튼\n",
    "        generate_button.click(\n",
    "            fn=rag_chain,\n",
    "            inputs=[url_input, query_input],\n",
    "            outputs=[answer_output, shared_state],  # 답변 결과와 shared_state에 저장\n",
    "        )\n",
    "\n",
    "    # Tab 2: 시각화 (워드클라우드)\n",
    "    with gr.Tab(\"음성파일 생성(워드클라우드)\"):\n",
    "        shared_textbox = gr.Textbox(label=\"요청 결과 내용\", interactive=False)\n",
    "        language_dropdown = gr.Dropdown(\n",
    "            choices=[\"ko\", \"en\", \"ja\", \"zh-cn\"], label=\"언어 선택\", value=\"ko\"\n",
    "        )\n",
    "        audio_output = gr.Audio(label=\"생성된 오디오\")\n",
    "        status_message = gr.Textbox(label=\"상태 메시지\")\n",
    "        convert_button = gr.Button(\"변환 시작\")\n",
    "\n",
    "        # shared_state 값을 shared_textbox에 업데이트\n",
    "        shared_state.change(fn=lambda x: x, inputs=shared_state, outputs=shared_textbox)\n",
    "\n",
    "        # Text-to-Speech 호출\n",
    "        convert_button.click(\n",
    "            fn=process_tts,\n",
    "            inputs=[shared_textbox, language_dropdown],\n",
    "            outputs=[audio_output, status_message],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 디버그 모드로 Gradio 인터페이스 실행\n",
    "iface.launch(server_port=7861, server_name=\"0.0.0.0\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "iface.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
